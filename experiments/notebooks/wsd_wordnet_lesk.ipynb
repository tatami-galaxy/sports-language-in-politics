{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38b57903-fa34-4c72-b802-9b511b30aa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /users/ujan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /users/ujan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet2022 to\n",
      "[nltk_data]     /users/ujan/nltk_data...\n",
      "[nltk_data]   Package wordnet2022 is already up-to-date!\n",
      "span-marker is already registered. Overwriting pipeline for task span-marker...\n"
     ]
    }
   ],
   "source": [
    "from nltk import wsd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from spacy.cli import download\n",
    "from spacy import load\n",
    "import warnings\n",
    "import json\n",
    "import re\n",
    "import polars as pl\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('wordnet2022')\n",
    "#!python -m spacy download en\n",
    "nlp = load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bcac7ed-086a-4e51-978a-6d678d4c0abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 'The die is cast.'\n",
    "Y = 'Roll the die to get a 6.'\n",
    "Z = 'What is dead may never die.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e60559b-5ae3-4278-9dcd-203fe8840c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('die.n.01'),\n",
       " Synset('die.n.02'),\n",
       " Synset('die.n.03'),\n",
       " Synset('die.v.01'),\n",
       " Synset('die.v.02'),\n",
       " Synset('die.v.03'),\n",
       " Synset('fail.v.04'),\n",
       " Synset('die.v.05'),\n",
       " Synset('die.v.06'),\n",
       " Synset('die.v.07'),\n",
       " Synset('die.v.08'),\n",
       " Synset('die.v.09'),\n",
       " Synset('die.v.10'),\n",
       " Synset('die.v.11')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"die\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8b55625-b748-4b4d-ae98-ae519e1f027f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defination 0 : a small cube with 1 to 6 spots on the six faces; used in gambling to generate random numbers\n",
      "defination 1 : a device used for shaping metal\n",
      "defination 2 : a cutting tool that is fitted into a diestock and used for cutting male (external) screw threads on screws or bolts or pipes or rods\n"
     ]
    }
   ],
   "source": [
    "# print all the definations of nouns\n",
    "i =0\n",
    "for syn in wn.synsets('die', pos=wn.NOUN):\n",
    "    print(\"defination {0} : {1}\".format(i, syn.definition()))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdce9385-0039-4926-b8a9-3167b0bd1fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The die is cast.\n",
      "Synset('die.v.07')\n",
      "cut or shape with a die\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(wsd.lesk(X.split(), 'die'))\n",
    "print(wsd.lesk(X.split(), 'die').definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c08ecd8-ce32-47fa-a560-c8f1fbf60440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('prize.n.01'),\n",
       " Synset('loot.n.01'),\n",
       " Synset('trophy.n.02'),\n",
       " Synset('prize.v.01'),\n",
       " Synset('pry.v.01'),\n",
       " Synset('respect.v.01'),\n",
       " Synset('choice.s.01')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"prize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1b71c5d2-81a3-4678-88d3-676432dd31ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defination 0 : education imparted in a series of lessons or meetings\n",
      "defination 1 : a connected series of events or actions or developments\n",
      "defination 2 : general line of orientation\n",
      "defination 3 : a mode of action\n",
      "defination 4 : a line or route along which something travels or moves\n",
      "defination 5 : a body of students who are taught together\n",
      "defination 6 : part of a meal served at one time\n",
      "defination 7 : (construction) a layer of masonry\n",
      "defination 8 : facility consisting of a circumscribed area of land or water laid out for a sport\n"
     ]
    }
   ],
   "source": [
    "# print all the definations of nouns\n",
    "i =0\n",
    "for syn in wn.synsets('course', pos=wn.NOUN):\n",
    "    print(\"defination {0} : {1}\".format(i, syn.definition()))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "32b6a095-f4fb-4577-9b9d-6b8c82a0e271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defination 0 : cause to move by pulling\n",
      "defination 1 : get or derive\n",
      "defination 2 : make a mark or lines on a surface\n",
      "defination 3 : make, formulate, or derive in the mind\n",
      "defination 4 : bring, take, or pull out of a container or from under a cover\n",
      "defination 5 : represent by making a drawing of, as with a pencil, chalk, etc. on a surface\n",
      "defination 6 : take liquid out of a container or well\n",
      "defination 7 : give a description of\n",
      "defination 8 : select or take in from a given group or region\n",
      "defination 9 : elicit responses, such as objections, criticism, applause, etc.\n",
      "defination 10 : suck in or take (air)\n",
      "defination 11 : move or go steadily or gradually\n",
      "defination 12 : remove (a commodity) from (a supply source)\n",
      "defination 13 : choose at random\n",
      "defination 14 : earn or achieve a base by being walked by the pitcher\n",
      "defination 15 : bring or lead someone to a certain action or condition\n",
      "defination 16 : cause to flow\n",
      "defination 17 : write a legal document or paper\n",
      "defination 18 : engage in drawing\n",
      "defination 19 : move or pull so as to cover or uncover something\n",
      "defination 20 : allow a draft\n",
      "defination 21 : require a specified depth for floating\n",
      "defination 22 : pull (a person) apart with four horses tied to his extremities, so as to execute him\n",
      "defination 23 : cause to move in a certain direction by exerting a force upon, either physically or in an abstract sense\n",
      "defination 24 : take in, also metaphorically\n",
      "defination 25 : direct toward itself or oneself by means of some psychological power or physical attributes\n",
      "defination 26 : thread on or as if on a string\n",
      "defination 27 : stretch back a bowstring (on an archer's bow)\n",
      "defination 28 : pass over, across, or through\n",
      "defination 29 : finish a game with an equal number of points, goals, etc.\n",
      "defination 30 : contract\n",
      "defination 31 : reduce the diameter of (a wire or metal rod) by pulling it through a die\n",
      "defination 32 : steep; pass through a strainer\n",
      "defination 33 : remove the entrails of\n",
      "defination 34 : flatten, stretch, or mold metal or glass, by rolling or by pulling it through a die or by stretching\n",
      "defination 35 : cause to localize at one point\n"
     ]
    }
   ],
   "source": [
    "# print all the definations of nouns\n",
    "i =0\n",
    "for syn in wn.synsets('draw', pos=wn.VERB):\n",
    "    print(\"defination {0} : {1}\".format(i, syn.definition()))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81a293fb-d4e7-4a34-9d07-5c1c0aa9becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_MAP = {\n",
    "    'VERB': wn.VERB,\n",
    "    'NOUN': wn.NOUN,\n",
    "    'PROPN': wn.NOUN\n",
    "}\n",
    "\n",
    "\n",
    "def lesk(doc, word):\n",
    "    found = False\n",
    "    for token in doc:\n",
    "        if token.text == word:\n",
    "            word = token\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        raise ValueError(f'Word \\\"{word}\\\" does not appear in the document: {doc.text}.')\n",
    "    pos = POS_MAP.get(word.pos_, False)\n",
    "    if not pos:\n",
    "        warnings.warn(f'POS tag for {word.text} not found in wordnet. Falling back to default Lesk behaviour.')\n",
    "    args = [c.text for c in doc], word.text\n",
    "    kwargs = dict(pos=pos)\n",
    "    return wsd.lesk(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e0d9a73-caef-4ade-97a3-285911eec4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('die.v.07')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('What is dead may never die.')\n",
    "lesk(doc, 'die')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa586a54-f2c8-45d6-9d7f-9dca36c6f608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cut or shape with a die'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesk(doc, 'die').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "603de7bb-5150-4231-9230-60858fa51348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a widely used search engine that uses text-matching techniques to find web pages that are important and relevant to a user's search\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesk(nlp('I work at google.'), 'google').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a42979e-31fb-4139-9d26-f2d1ca481658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'search the internet (for information) using the Google search engine'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesk(nlp('I will google it.'), 'google').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fa5c8d9-7bed-4b91-9360-01256c6641b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'intend with some possibility of fulfilment'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesk(nlp('I hope we win!'), 'hope').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "003dcb92-0294-4922-bd27-92dc1a1fe6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/users/ujan/Downloads/exact_matches_politics_42_10000_v2_latest.json') as f:\n",
    "    pol_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63a0aa52-0e6e-48eb-b52e-b3af6449f657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cx29aad',\n",
       " 'dfn34if',\n",
       " 'dho9j4b',\n",
       " 'eri5wv8',\n",
       " 'ct2h93v',\n",
       " 'dedn867',\n",
       " 'fs1c2dp',\n",
       " 'g5fnns1',\n",
       " 'dhh6o99',\n",
       " 'dbmnncp',\n",
       " 'cgk72ox',\n",
       " 'fic4amk',\n",
       " 'ci6s2dk',\n",
       " 'cihkiev',\n",
       " 'e33fq0a',\n",
       " 'e33fq0a',\n",
       " 'e33fq0a',\n",
       " 'e33fq0a',\n",
       " 'e33fq0a',\n",
       " 'e33fq0a',\n",
       " 'dg4iw5p',\n",
       " 'cger8i9',\n",
       " 'fzpyjy6',\n",
       " 'fci1xv2',\n",
       " 'fofbo3d',\n",
       " 'dznko8q',\n",
       " 'gh79bgx',\n",
       " 'fmoka4x',\n",
       " 'ef0sted',\n",
       " 'fq5rg2l',\n",
       " 'f5oysxv',\n",
       " 'h07ql5b',\n",
       " 'e68dugz',\n",
       " 'd4elksb',\n",
       " 'g7xz8bb',\n",
       " 'fp37o38',\n",
       " 'fj8zlex',\n",
       " 'cli5zdx',\n",
       " 'h31hy41',\n",
       " 'cf69n26',\n",
       " 'eodh4h0',\n",
       " 'hgct5i2',\n",
       " 'd6hcpdo',\n",
       " 'hbhgxbl',\n",
       " 'daa9zma',\n",
       " 'e3o8cm2',\n",
       " 'ep16pxp',\n",
       " 'dpjrjcu',\n",
       " 'f4ona9f',\n",
       " 'el4fggb',\n",
       " 'duxliwd',\n",
       " 'cro7o9v',\n",
       " 'gx17nhc',\n",
       " 'fx1wlek',\n",
       " 'gy06d12',\n",
       " 'dvdxyvs',\n",
       " 'dxgrvca',\n",
       " 'f7dyatk',\n",
       " 'daf9m1n',\n",
       " 'g35976g',\n",
       " 'd1ouezr',\n",
       " 'evc8jlb',\n",
       " 'e4m7lxx',\n",
       " 'g7boje5',\n",
       " 'd19onlp',\n",
       " 'dht86g5',\n",
       " 'daxu4g2',\n",
       " 'cglpso8',\n",
       " 'frwf2wf',\n",
       " 'fe6fxoz',\n",
       " 'gq74xfg',\n",
       " 'e0fuk3n',\n",
       " 'd9vzcgz',\n",
       " 'efoatmm',\n",
       " 'daf903z',\n",
       " 'dudtq65',\n",
       " 'e8x8qkr',\n",
       " 'fz1o3ym',\n",
       " 'd0e7xg0',\n",
       " 'dp28x8x',\n",
       " 'gfb5gvw',\n",
       " 'd099gqx',\n",
       " 'd6wb0gk',\n",
       " 'ernpvwn',\n",
       " 'e108q9j',\n",
       " 'flu9gnl',\n",
       " 'girr6zw',\n",
       " 'fglcjpb',\n",
       " 'dxspfry',\n",
       " 'frw6ha3',\n",
       " 'ddiqbqs',\n",
       " 'd8rdsjn',\n",
       " 'g87g425',\n",
       " 'g87g425',\n",
       " 'g3v8g9z',\n",
       " 'dgrzufk',\n",
       " 'ewdp54f',\n",
       " 'd2khxog',\n",
       " 'g4of8en',\n",
       " 'cuca6db',\n",
       " 'eur8epu',\n",
       " 'fe6is8z',\n",
       " 'fe6is8z',\n",
       " 'f17ttrg',\n",
       " 'gv9bfhf',\n",
       " 'gjl1dia',\n",
       " 'cjs9j5f',\n",
       " 'cjs9j5f',\n",
       " 'df67g85',\n",
       " 'd02l6so',\n",
       " 'co68k2i',\n",
       " 'd499085',\n",
       " 'f7txm94',\n",
       " 'd77r2bs',\n",
       " 'eywivit',\n",
       " 'dssrs2u',\n",
       " 'fkp6bl9',\n",
       " 'dbtkxed',\n",
       " 'd90mjr3',\n",
       " 'ec7dkp7',\n",
       " 'd0c6jru',\n",
       " 'fje3xdz',\n",
       " 'fkrxaek',\n",
       " 'dd0rhf2',\n",
       " 'djk4sd2',\n",
       " 'ealzst5',\n",
       " 'era1pgp',\n",
       " 'ct2iydw',\n",
       " 'f7l7kdc',\n",
       " 'gnyramg',\n",
       " 'dhvhv2f',\n",
       " 'd4f4wc6',\n",
       " 'enuc5fy',\n",
       " 'hgcvut2',\n",
       " 'ec97wnr',\n",
       " 'ew2qdzb',\n",
       " 'dy8z8wm',\n",
       " 'ebxut74',\n",
       " 'hqcq6su',\n",
       " 'daoy9ob',\n",
       " 'gowjjx5',\n",
       " 'fimldzu',\n",
       " 'd1gazvu',\n",
       " 'easxk2c',\n",
       " 'd0njbzt',\n",
       " 'dx37eqh',\n",
       " 'dx37eqh',\n",
       " 'h16qvdf',\n",
       " 'dptj5y5',\n",
       " 'd23gdua',\n",
       " 'cvzn76s',\n",
       " 'e9tc91o',\n",
       " 'd8kkmds',\n",
       " 'dcjunkw',\n",
       " 'dcjunkw',\n",
       " 'dcp1itj',\n",
       " 'csnj1rl',\n",
       " 'dov9l7s',\n",
       " 'g9lzfkt',\n",
       " 'elw43d2',\n",
       " 'eub6zej',\n",
       " 'e1obkbu',\n",
       " 'ch4fww8',\n",
       " 'd1y9pcz',\n",
       " 'd9ieoff',\n",
       " 'd9ieoff',\n",
       " 'fwjkbs8',\n",
       " 'dpi45gp',\n",
       " 'eghx42s',\n",
       " 'dcxmxgl',\n",
       " 'd01fovg',\n",
       " 'eakrlis',\n",
       " 'eakrlis',\n",
       " 'emasvyd',\n",
       " 'h07btxb',\n",
       " 'go6apac',\n",
       " 'ez99y3t',\n",
       " 'cngrei6',\n",
       " 'e5vqpfb',\n",
       " 'fqzw1f3',\n",
       " 'eh4n6rt',\n",
       " 'gw5vgu1',\n",
       " 'gw5vgu1',\n",
       " 'd4cp5td',\n",
       " 'djfk0i2',\n",
       " 'ffc0pel',\n",
       " 'd20xjfw',\n",
       " 'gkkh8ch',\n",
       " 'gkkh8ch',\n",
       " 'fmglhs8',\n",
       " 'fc0sj6y',\n",
       " 'h4gm5gw',\n",
       " 'hkx0lkr',\n",
       " 'fm588gg',\n",
       " 'fbbmcdk',\n",
       " 'fbbmcdk',\n",
       " 'fbbmcdk',\n",
       " 'gnjyo5o',\n",
       " 'chetszc',\n",
       " 'fkkfz82',\n",
       " 'duzb00n',\n",
       " 'gb0mxvc',\n",
       " 'hbrs7dw',\n",
       " 'es9htj2',\n",
       " 'd7wjyn7',\n",
       " 'dqr3qk7',\n",
       " 'emtiq6x',\n",
       " 'e7q4gw3',\n",
       " 'gajaala',\n",
       " 'e801yah',\n",
       " 'dnhdlbt',\n",
       " 'g8ryhrv',\n",
       " 'f4rgngc',\n",
       " 'gebvm12',\n",
       " 'd6m9mkd',\n",
       " 'eeho116',\n",
       " 'fmuy78s',\n",
       " 'gy10exf',\n",
       " 'h3bj2u6',\n",
       " 'ginbglr',\n",
       " 'hni7w9t',\n",
       " 'fijdn6h',\n",
       " 'h1gdbn4',\n",
       " 'dn5nbqu',\n",
       " 'dgkxfqw',\n",
       " 'ehp6x72',\n",
       " 'dl65ibe',\n",
       " 'h5k2ldl',\n",
       " 'dcscyd6',\n",
       " 'g14kn1x',\n",
       " 'fjxqcdm',\n",
       " 'hcchu7w',\n",
       " 'ffkhrby',\n",
       " 'cghf1a8',\n",
       " 'hf4vfgn']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prize_ids = pol_data['course'][1:][0]\n",
    "prize_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8c73931-40ff-4448-ab84-e7f8c166089a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>created_utc</th><th>subreddit</th><th>category</th><th>super_category</th><th>body</th><th>author</th></tr><tr><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;er1nbsy&quot;</td><td>1560434110</td><td>&quot;AOC&quot;</td><td>&quot;politics_2019&quot;</td><td>&quot;politics&quot;</td><td>&quot;Lol. I was ask…</td><td>&quot;Just_WoW_Thing…</td></tr><tr><td>&quot;gq5rux6&quot;</td><td>1615164573</td><td>&quot;AOC&quot;</td><td>&quot;politics_2019&quot;</td><td>&quot;politics&quot;</td><td>&quot;stfu Liberal.&quot;</td><td>&quot;gbsedillo20&quot;</td></tr><tr><td>&quot;gj3jztp&quot;</td><td>1610531437</td><td>&quot;AOC&quot;</td><td>&quot;politics_2019&quot;</td><td>&quot;politics&quot;</td><td>&quot;\n",
       "&amp;gt;Fuck that…</td><td>&quot;64590949354397…</td></tr><tr><td>&quot;esrc8yq&quot;</td><td>1562221646</td><td>&quot;AOC&quot;</td><td>&quot;politics_2019&quot;</td><td>&quot;politics&quot;</td><td>&quot;Cauliflower is…</td><td>&quot;TobiKato&quot;</td></tr><tr><td>&quot;gyjpk7u&quot;</td><td>1621327024</td><td>&quot;AOC&quot;</td><td>&quot;politics_2019&quot;</td><td>&quot;politics&quot;</td><td>&quot;If they build …</td><td>&quot;the_lonely_gam…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "┌─────────┬─────────────┬───────────┬───────────────┬───────────────┬───────────────┬──────────────┐\n",
       "│ id      ┆ created_utc ┆ subreddit ┆ category      ┆ super_categor ┆ body          ┆ author       │\n",
       "│ ---     ┆ ---         ┆ ---       ┆ ---           ┆ y             ┆ ---           ┆ ---          │\n",
       "│ str     ┆ i64         ┆ str       ┆ str           ┆ ---           ┆ str           ┆ str          │\n",
       "│         ┆             ┆           ┆               ┆ str           ┆               ┆              │\n",
       "╞═════════╪═════════════╪═══════════╪═══════════════╪═══════════════╪═══════════════╪══════════════╡\n",
       "│ er1nbsy ┆ 1560434110  ┆ AOC       ┆ politics_2019 ┆ politics      ┆ Lol. I was    ┆ Just_WoW_Thi │\n",
       "│         ┆             ┆           ┆               ┆               ┆ asking about  ┆ ngs          │\n",
       "│         ┆             ┆           ┆               ┆               ┆ the othe…     ┆              │\n",
       "│ gq5rux6 ┆ 1615164573  ┆ AOC       ┆ politics_2019 ┆ politics      ┆ stfu Liberal. ┆ gbsedillo20  │\n",
       "│ gj3jztp ┆ 1610531437  ┆ AOC       ┆ politics_2019 ┆ politics      ┆               ┆ 645909493543 │\n",
       "│         ┆             ┆           ┆               ┆               ┆ &gt;Fuck      ┆ 97548569     │\n",
       "│         ┆             ┆           ┆               ┆               ┆ that, no it's ┆              │\n",
       "│         ┆             ┆           ┆               ┆               ┆ not. The…     ┆              │\n",
       "│ esrc8yq ┆ 1562221646  ┆ AOC       ┆ politics_2019 ┆ politics      ┆ Cauliflower   ┆ TobiKato     │\n",
       "│         ┆             ┆           ┆               ┆               ┆ is racist. �� ┆              │\n",
       "│         ┆             ┆           ┆               ┆               ┆ Such w…       ┆              │\n",
       "│ gyjpk7u ┆ 1621327024  ┆ AOC       ┆ politics_2019 ┆ politics      ┆ If they build ┆ the_lonely_g │\n",
       "│         ┆             ┆           ┆               ┆               ┆ the third     ┆ ame          │\n",
       "│         ┆             ┆           ┆               ┆               ┆ temple, …     ┆              │\n",
       "└─────────┴─────────────┴───────────┴───────────────┴───────────────┴───────────────┴──────────────┘"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pl.read_csv('/users/ujan/sports-language-in-politics/data/processed/politics_sample.csv').drop_nulls()\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da02f1ff-d787-4bf9-a8db-4877f27d2074",
   "metadata": {},
   "outputs": [],
   "source": [
    "prize_comments = data_df.filter(pl.col('id').is_in(prize_ids))['body'].to_list()\n",
    "prize_comments = [re.sub(r\"[^a-zA-Z0-9]+\", ' ', comment).lower() for comment in prize_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "314720f4-9667-4881-a8f6-1be6feb3dbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that s exactly it without education 0 we re below average animals they run away earlier than us usually that s also one reason people mentionned about east asia numbers they had sars to learn and remember 1 so their reaction was clear and quick while the west took a month or more of swinging left and right until the conclusion drew itself on the board 0 of course epidemiologist and medical staff have some but they re not authorities and people barely respect authorities so here you go 1 interesting fractal effect society members memory is an immune system and covid 19 is our first flu shot '"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prize_comments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5ee822fd-f0ca-4d23-a316-999d9969d4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of course you claim its that high your a trump supporter the polls are all over the place because right wing polls claim he is more popular then it is trump supporters are the scum of the earth on level with isis if trump is over 35 i would be surprised '"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prize_comments[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "16368497-f963-479c-a694-4377dbbbfff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'facility consisting of a circumscribed area of land or water laid out for a sport'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesk(nlp('of course epidemiologist and medical staff have some but they re not authorities and people barely respect authorities'), 'course').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ed8d1e03-098c-4a13-88dd-1bc733becbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a body of students who are taught together'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesk(nlp('of course you claim its that high your a trump supporter the polls are all over the place because right wing polls claim he is more popular then it is trump supporters are the scum of the earth on level with isis if trump is over 35 i would be surprised '), 'course').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f2ad5f-9fd2-4d8c-8890-3c534f7a173f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32257958-1851-4d5c-9b03-e68d6db2f434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aa8433c9-767b-42a7-991f-7e155c032923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "86b81395-591f-4dca-9183-07790ccbb85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34285ff957ac44dea411b24aba236917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29486c7ebcb4cb6a5eb332b356e82e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6c1ee336ae41d8820eaf8a7d788565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d6e6a6f0604d86bdd640943a306013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0719e3d1794641aa910111d6cea45ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/20138 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "semcor = load_dataset('MarkChen1214/SemCor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dbe398b7-1b83-4b39-8872-01bc918b0d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'Word', 'Lemma', 'POS', 'Definition', 'Lemma_sentence', 'sentence', 'Lemma_tfidf', 'Lemma_tfidf_value'],\n",
       "        num_rows: 20138\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "abbe8b4d-0b54-4ba6-adda-6f17192701ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Polyphosphates gave renewed life to soap products at a time when surfactants were a threat though expensive , and these same polyphosphates spelled the decline of soap usage when the synergism between polyphosphates and synthetic detergent actives was recognized and exploited .'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor['train'][0]['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "af16938a-320c-4cd8-b544-62ca4b408ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resisting weight or pressure',\n",
       " 'the outer boundary of an artifact or a material layer constituting or resembling such a boundary',\n",
       " 'a preparation used in cleaning something',\n",
       " 'to consider or examine in speech or writing',\n",
       " 'a subdivision of a written work; usually numbered and titled',\n",
       " 'coming next after the twenty-seventh in position']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor['train'][10]['Definition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fd08e731-260f-4491-a1a9-366cf9272105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Polyphosphates',\n",
       " 'gave',\n",
       " 'renewed',\n",
       " 'life',\n",
       " 'soap',\n",
       " 'products',\n",
       " 'time',\n",
       " 'surfactants',\n",
       " 'were',\n",
       " 'threat',\n",
       " 'expensive',\n",
       " 'same',\n",
       " 'polyphosphates',\n",
       " 'spelled',\n",
       " 'decline',\n",
       " 'soap',\n",
       " 'usage',\n",
       " 'synergism',\n",
       " 'polyphosphates',\n",
       " 'synthetic',\n",
       " 'detergent',\n",
       " 'actives',\n",
       " 'recognized',\n",
       " 'exploited']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor['train'][0]['Word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6053781-e6da-411e-8c05-4068060c5068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f01117b-f678-4efa-8272-bb75a1888cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "226665f7-7fc4-4d8e-b7ab-17c20761ba26",
   "metadata": {},
   "source": [
    "#### WSD with BERT MASK probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e67a369-67c8-4cac-89b4-be0e46c02441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9807140b-9241-4b57-998f-719ec294507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb891b1-1856-4b0a-ad8c-a0f7cac9beeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77017125-fafd-4db1-887d-16d0f63601e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_words = []\n",
    "for w in wn.all_synsets():\n",
    "    if 'sports' in w.definition():\n",
    "        sports_words.extend(w.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed624b2b-eb5d-4417-ae98-c1ee2a2d4f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sports_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "538db044-46bf-45fb-8377-7545b47313f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clean',\n",
       " 'sporting',\n",
       " 'sporty',\n",
       " 'sportsmanlike',\n",
       " 'sportive',\n",
       " 'sporting',\n",
       " 'unsportingly',\n",
       " 'fumble',\n",
       " 'muff',\n",
       " 'pass',\n",
       " 'toss',\n",
       " 'flip',\n",
       " 'place_kick',\n",
       " 'place-kicking',\n",
       " 'tag',\n",
       " 'call',\n",
       " 'hat_trick',\n",
       " 'track_and_field',\n",
       " 'water_sport',\n",
       " 'aquatics',\n",
       " 'sporting_life',\n",
       " 'victory_celebration',\n",
       " 'play',\n",
       " 'assist',\n",
       " 'stroke',\n",
       " 'shot',\n",
       " 'cut',\n",
       " 'undercut',\n",
       " 'drive',\n",
       " 'forehand',\n",
       " 'forehand_stroke',\n",
       " 'forehand_shot',\n",
       " 'forehand_drive',\n",
       " 'serve',\n",
       " 'service',\n",
       " 'fault',\n",
       " 'position',\n",
       " 'technical_foul',\n",
       " 'technical',\n",
       " 'athletics',\n",
       " 'replay',\n",
       " 'instant_replay',\n",
       " 'action_replay',\n",
       " 'save',\n",
       " 'rally',\n",
       " 'exchange',\n",
       " 'cheap_shot',\n",
       " 'dirty_pool',\n",
       " 'sporting_dog',\n",
       " 'gun_dog']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sports_words[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "387cbacf-db5c-4586-9976-16bb85a89608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for word in sports_words:\n",
    "    if word in sorted_token_probs:\n",
    "        count += 1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a8d355a-f5ea-47d7-9258-08d95a390e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.029426940073832952\n",
      "0.014713527005746851\n"
     ]
    }
   ],
   "source": [
    "TARGET = 'course'\n",
    "\n",
    "comments = [\n",
    "    'I will be at the course later',\n",
    "    'Of course I will see you later'\n",
    "]\n",
    "\n",
    "token_dict = {}\n",
    "for comment in comments:\n",
    "    tokens = comment.split()\n",
    "    tokens = ['[MASK]' if token == TARGET else token for token in tokens]\n",
    "    masked_comment = ' '.join(tokens)\n",
    "    inputs = tokenizer(masked_comment, return_tensors=\"pt\", truncation=True)\n",
    "    if inputs['input_ids'].shape[-1] >= 512:\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        # get mask token ids\n",
    "        mask_token_ids = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "        # for each masked token (multiple biden mentions)\n",
    "        for mask_token_id in mask_token_ids:\n",
    "            # get corresponding logits\n",
    "            mask_logits = logits[0, mask_token_id.item()]\n",
    "            # softmax to get probs for each token in vocab\n",
    "            mask_probs = torch.nn.functional.softmax(mask_logits, dim=0)\n",
    "            # sort in descending order\n",
    "            prob_values, token_ids = torch.sort(mask_probs, descending=True)\n",
    "            # for each token in vocab\n",
    "            for i in range(len(token_ids)):\n",
    "                # decode to get string\n",
    "                token = tokenizer.decode(token_ids[i])\n",
    "                # prob value\n",
    "                value = prob_values[i].item()\n",
    "                \n",
    "                if token in token_dict:\n",
    "                    token_dict[token]['value'] += value\n",
    "                    token_dict[token]['count'] += 1\n",
    "                else:\n",
    "                    token_dict[token] = {'value': value, 'count': 1}\n",
    "\n",
    "    new_token_dict = {}\n",
    "    for key, val in token_dict.items():\n",
    "        new_token_dict[key] = val['value'] / val['count']\n",
    "\n",
    "    sorted_token_probs = dict(sorted(new_token_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    prob_value = 0\n",
    "    for word in sports_words:\n",
    "        if word in sorted_token_probs:\n",
    "            prob_value += sorted_token_probs[word]\n",
    "    print(prob_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93828994-7c7a-4240-a320-ee04fb54e3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005403707188477158\n",
      "0.0002809585464422058\n"
     ]
    }
   ],
   "source": [
    "TARGET = 'card'\n",
    "\n",
    "comments = [\n",
    "    'He was shown a red card for the tackle',\n",
    "    'My credit card expired yesterday'\n",
    "]\n",
    "\n",
    "token_dict = {}\n",
    "for comment in comments:\n",
    "    tokens = comment.split()\n",
    "    tokens = ['[MASK]' if token == TARGET else token for token in tokens]\n",
    "    masked_comment = ' '.join(tokens)\n",
    "    inputs = tokenizer(masked_comment, return_tensors=\"pt\", truncation=True)\n",
    "    if inputs['input_ids'].shape[-1] >= 512:\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        # get mask token ids\n",
    "        mask_token_ids = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "        # for each masked token (multiple biden mentions)\n",
    "        for mask_token_id in mask_token_ids:\n",
    "            # get corresponding logits\n",
    "            mask_logits = logits[0, mask_token_id.item()]\n",
    "            # softmax to get probs for each token in vocab\n",
    "            mask_probs = torch.nn.functional.softmax(mask_logits, dim=0)\n",
    "            # sort in descending order\n",
    "            prob_values, token_ids = torch.sort(mask_probs, descending=True)\n",
    "            # for each token in vocab\n",
    "            for i in range(len(token_ids)):\n",
    "                # decode to get string\n",
    "                token = tokenizer.decode(token_ids[i])\n",
    "                # prob value\n",
    "                value = prob_values[i].item()\n",
    "                \n",
    "                if token in token_dict:\n",
    "                    token_dict[token]['value'] += value\n",
    "                    token_dict[token]['count'] += 1\n",
    "                else:\n",
    "                    token_dict[token] = {'value': value, 'count': 1}\n",
    "\n",
    "    new_token_dict = {}\n",
    "    for key, val in token_dict.items():\n",
    "        new_token_dict[key] = val['value'] / val['count']\n",
    "\n",
    "    sorted_token_probs = dict(sorted(new_token_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    prob_value = 0\n",
    "    for word in sports_words:\n",
    "        if word in sorted_token_probs:\n",
    "            prob_value += sorted_token_probs[word]\n",
    "    print(prob_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c20fa56-e9e8-407a-bac5-b5e0a77a624f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.286751383745389\n",
      "0.15239340089198766\n"
     ]
    }
   ],
   "source": [
    "TARGET = 'fan'\n",
    "\n",
    "comments = [\n",
    "    'He was a life long fan of his local club',\n",
    "    'His actions only served to fan the flames',\n",
    "]\n",
    "\n",
    "token_dict = {}\n",
    "for comment in comments:\n",
    "    tokens = comment.split()\n",
    "    tokens = ['[MASK]' if token == TARGET else token for token in tokens]\n",
    "    masked_comment = ' '.join(tokens)\n",
    "    inputs = tokenizer(masked_comment, return_tensors=\"pt\", truncation=True)\n",
    "    if inputs['input_ids'].shape[-1] >= 512:\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        # get mask token ids\n",
    "        mask_token_ids = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "        # for each masked token (multiple biden mentions)\n",
    "        for mask_token_id in mask_token_ids:\n",
    "            # get corresponding logits\n",
    "            mask_logits = logits[0, mask_token_id.item()]\n",
    "            # softmax to get probs for each token in vocab\n",
    "            mask_probs = torch.nn.functional.softmax(mask_logits, dim=0)\n",
    "            # sort in descending order\n",
    "            prob_values, token_ids = torch.sort(mask_probs, descending=True)\n",
    "            # for each token in vocab\n",
    "            for i in range(len(token_ids)):\n",
    "                # decode to get string\n",
    "                token = tokenizer.decode(token_ids[i])\n",
    "                # prob value\n",
    "                value = prob_values[i].item()\n",
    "                \n",
    "                if token in token_dict:\n",
    "                    token_dict[token]['value'] += value\n",
    "                    token_dict[token]['count'] += 1\n",
    "                else:\n",
    "                    token_dict[token] = {'value': value, 'count': 1}\n",
    "\n",
    "    new_token_dict = {}\n",
    "    for key, val in token_dict.items():\n",
    "        new_token_dict[key] = val['value'] / val['count']\n",
    "\n",
    "    sorted_token_probs = dict(sorted(new_token_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    prob_value = 0\n",
    "    for word in sports_words:\n",
    "        if word in sorted_token_probs:\n",
    "            prob_value += sorted_token_probs[word]\n",
    "    print(prob_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf4a45-ca94-4d4f-99ec-b98d01d0d89b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396dfb57-6e86-49e9-8c2e-1b69b22ac6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9be126d3-10c6-447b-9ebf-cb942bb7e6d2",
   "metadata": {},
   "source": [
    "##### 'context free' probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aa4f757-d18e-42f5-b432-e6457567d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'word'\n",
    "\n",
    "\n",
    "# do this for important words #\n",
    "comments = [\n",
    "    '[PAD] word [PAD]',\n",
    "    #'My credit card expired yesterday'\n",
    "]\n",
    "\n",
    "token_dict = {}\n",
    "for comment in comments:\n",
    "    tokens = comment.split()\n",
    "    tokens = ['[MASK]' if token == TARGET else token for token in tokens]\n",
    "    masked_comment = ' '.join(tokens)\n",
    "    inputs = tokenizer(masked_comment, return_tensors=\"pt\", truncation=True)\n",
    "    if inputs['input_ids'].shape[-1] >= 512:\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        # get mask token ids\n",
    "        mask_token_ids = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "        # for each masked token (multiple biden mentions)\n",
    "        for mask_token_id in mask_token_ids:\n",
    "            # get corresponding logits\n",
    "            mask_logits = logits[0, mask_token_id.item()]\n",
    "            # softmax to get probs for each token in vocab\n",
    "            mask_probs = torch.nn.functional.softmax(mask_logits, dim=0)\n",
    "            # sort in descending order\n",
    "            prob_values, token_ids = torch.sort(mask_probs, descending=True)\n",
    "            # for each token in vocab\n",
    "            for i in range(len(token_ids)):\n",
    "                # decode to get string\n",
    "                token = tokenizer.decode(token_ids[i])\n",
    "                # prob value\n",
    "                value = prob_values[i].item()\n",
    "                \n",
    "                if token in token_dict:\n",
    "                    token_dict[token]['value'] += value\n",
    "                    token_dict[token]['count'] += 1\n",
    "                else:\n",
    "                    token_dict[token] = {'value': value, 'count': 1}\n",
    "\n",
    "    new_token_dict = {}\n",
    "    for key, val in token_dict.items():\n",
    "        new_token_dict[key] = val['value'] / val['count']\n",
    "\n",
    "    sorted_token_probs = dict(sorted(new_token_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    #print(sorted_token_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9025aed2-dd29-4f7e-b327-46738a2e957c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "election 1.6974567584870925e-10\n",
      "race 9.876374718054137e-11\n",
      "fans 6.21402374001434e-11\n",
      "voters 1.1286636555918328e-11\n"
     ]
    }
   ],
   "source": [
    "# word\n",
    "\n",
    "words = ['race', 'election', 'fans', 'voters']\n",
    "\n",
    "for key, val in sorted_token_probs.items():\n",
    "    if key in words:\n",
    "        print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72bc044c-e11e-4bc5-812a-d7e1f30cb6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voters 2.48651190304372e-06\n",
      "election 1.96735300050932e-06\n",
      "race 4.895503593616013e-07\n",
      "fans 3.7076819126014016e-07\n"
     ]
    }
   ],
   "source": [
    "# pad word pad\n",
    "\n",
    "words = ['race', 'election', 'fans', 'voters']\n",
    "\n",
    "for key, val in sorted_token_probs.items():\n",
    "    if key in words:\n",
    "        print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adf21a0-86d5-42be-8693-72538085911c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
