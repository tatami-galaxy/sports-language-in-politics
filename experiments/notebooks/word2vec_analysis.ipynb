{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "03375523-e310-4ccc-9476-c871d654f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from os.path import dirname, abspath\n",
    "from functools import partial\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "653b0dc3-bb80-47fb-bdc1-ff6fe02343df",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIMENSION = 300\n",
    "EMBED_MAX_NORM = 1\n",
    "\n",
    "data_dir = \"/users/ujan/sports-language-in-politics/models/cbow/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aad5ea58-9a8b-4b16-ae8f-acdd9b43e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available(): device = \"mps\"\n",
    "elif torch.cuda.is_available(): device = \"cuda\"\n",
    "else: device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a711a00-cd09-4b0a-bb90-5470f2146cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of CBOW model described in paper:\n",
    "    https://arxiv.org/abs/1301.3781\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super(CBOW_Model, self).__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=EMBED_DIMENSION,\n",
    "            max_norm=EMBED_MAX_NORM,\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=EMBED_DIMENSION,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.embeddings(inputs)\n",
    "        x = x.mean(axis=1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1d32dcae-bb05-426c-9c3e-98a41147a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_and_vocab(sub_name):\n",
    "    \n",
    "    vocab = torch.load(data_dir+sub_name+\"_vocab.pt\")\n",
    "    print('vocab size: {}'.format(len(vocab.get_itos())))\n",
    "    model = CBOW_Model(vocab_size=len(vocab.get_itos()))\n",
    "    model = torch.load(data_dir+sub_name+\"_model.pt\", map_location=device)\n",
    "\n",
    "    def normalization(embeddings):\n",
    "        norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\n",
    "        norms = np.reshape(norms, (len(norms), 1))\n",
    "        embeddings_norm = embeddings / norms\n",
    "        return embeddings_norm\n",
    "    \n",
    "    # embedding from first model layer\n",
    "    embeddings = list(model.parameters())[0].cpu().detach().numpy()\n",
    "    \n",
    "    # normalization\n",
    "    embeddings_norm = normalization(embeddings)\n",
    "\n",
    "    return embeddings_norm, vocab\n",
    "\n",
    "\n",
    "def get_top_similar(word: str, embeddings_norm, vocab, topN: int=5):\n",
    "    word_id = vocab[word]\n",
    "    if word_id == 0:\n",
    "        print(\"Out of vocabulary word\")\n",
    "        return\n",
    "\n",
    "    word_vec = embeddings_norm[word_id]\n",
    "    word_vec = np.reshape(word_vec, (len(word_vec), 1))\n",
    "    dists = np.matmul(embeddings_norm, word_vec).flatten()\n",
    "    topN_ids = np.argsort(-dists)[1 : topN + 1]\n",
    "\n",
    "    topN_dict = {}\n",
    "    for sim_word_id in topN_ids:\n",
    "        sim_word = vocab.lookup_token(sim_word_id)\n",
    "        topN_dict[sim_word] = dists[sim_word_id]\n",
    "    return topN_dict\n",
    "\n",
    "\n",
    "def get_sim(word1: str, word2: str, embeddings_norm, vocab):\n",
    "    if word1 not in vocab.get_itos() or word2 not in vocab.get_itos():\n",
    "        print('not in vocab')\n",
    "        return\n",
    "    word1_id = vocab[word1]\n",
    "    if word1_id == 0:\n",
    "        print(\"Out of vocabulary word\")\n",
    "        return\n",
    "    word2_id = vocab[word2]\n",
    "    if word2_id == 0:\n",
    "        print(\"Out of vocabulary word\")\n",
    "        return\n",
    "\n",
    "    word1_vec = embeddings_norm[word1_id]\n",
    "    #word1_vec = np.reshape(word1_vec, (len(word1_vec), 1))\n",
    "    \n",
    "    word2_vec = embeddings_norm[word2_id]\n",
    "    #word2_vec = np.reshape(word2_vec, (len(word2_vec), 1))\n",
    "    \n",
    "    print(np.dot(word1_vec, word2_vec))\n",
    "\n",
    "\n",
    "def get_analogy(a,b,c, embeddings_norm, vocab):\n",
    "    vocab_list = vocab.get_itos()\n",
    "    if a not in vocab_list or b not in vocab_list or c not in vocab_list:\n",
    "        print('not in vocab')\n",
    "        return\n",
    "    a_id = vocab[a]\n",
    "    b_id = vocab[b]\n",
    "    c_id = vocab[c]\n",
    "    if a_id == 0 or b_id == 0 or c_id == 0:\n",
    "        print(\"Out of vocabulary word\")\n",
    "        return\n",
    "    a_vec = embeddings_norm[a_id]\n",
    "    b_vec = embeddings_norm[b_id]\n",
    "    c_vec = embeddings_norm[c_id]\n",
    "    sim = -2\n",
    "    target = None\n",
    "    for token in vocab_list:\n",
    "        if token in [a,b,c]:\n",
    "            continue\n",
    "        token_id = vocab[token]\n",
    "        token_vec = embeddings_norm[token_id]\n",
    "        s = np.dot(token_vec, b_vec-a_vec+c_vec)\n",
    "        if s > sim:\n",
    "            sim = s\n",
    "            target = token\n",
    "    print(target, sim)\n",
    "\n",
    "\n",
    "def get_outlier(word_list, embeddings_norm, vocab):\n",
    "    vocab_list = vocab.get_itos()\n",
    "    for word in word_list:\n",
    "        if word not in vocab_list:\n",
    "            print('not in vocab')\n",
    "            return\n",
    "    pairs = list(itertools.combinations(word_list, 2))\n",
    "    sim_dict = {w:0 for w in word_list}\n",
    "    for pair in pairs:\n",
    "        vec1 = embeddings_norm[vocab[pair[0]]]\n",
    "        vec2 = embeddings_norm[vocab[pair[1]]]\n",
    "        sim = np.dot(vec1, vec2)\n",
    "        sim_dict[pair[0]] += sim\n",
    "        sim_dict[pair[1]] += sim\n",
    "    new_d = {w:0 for w in word_list}\n",
    "    for key, val in sim_dict.items():\n",
    "        new_d[key] = val/(len(word_list)-1)\n",
    "    word = None\n",
    "    value = 2\n",
    "    for key, val in new_d.items():\n",
    "        if val < value:\n",
    "            word = key\n",
    "            value = val\n",
    "    print(word, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51f1976-47a9-4d53-9eca-583d670161af",
   "metadata": {},
   "source": [
    "#### Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f11a9c1-a0db-4536-b4d2-e0b0fdcfc12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 12813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'terrible': 0.36026692,\n",
       " 'horrible': 0.3512573,\n",
       " 'shitty': 0.28138334,\n",
       " 'good': 0.25139344,\n",
       " 'awful': 0.23673256}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_norm, vocab = get_embeddings_and_vocab('the_donald_conservative') # freq 20\n",
    "get_top_similar('bad', embeddings_norm, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0e9f65a-d68c-44a8-8fab-5085bf1dfb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 10378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'despise': 0.4057723,\n",
       " 'dislike': 0.357977,\n",
       " 'hates': 0.33946168,\n",
       " 'love': 0.31733787,\n",
       " 'hated': 0.26700807}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_norm, vocab = get_embeddings_and_vocab('the_donald_1000_epochs') # freq 20, sample 100000\n",
    "get_top_similar('hate', embeddings_norm, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b78c0-abaa-48d0-8355-e96af091fbd6",
   "metadata": {},
   "source": [
    "#### Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6e819e6-09ce-4e50-8a89-18b265c4b9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 10378\n",
      "father 0.34129798\n"
     ]
    }
   ],
   "source": [
    "embeddings_norm, vocab = get_embeddings_and_vocab('the_donald_1000_epochs')\n",
    "get_analogy('woman', 'queen', 'man', embeddings_norm, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd17f3d1-e0d3-4778-accb-4e2bae562877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 10378\n",
      "bowls 0.34564084\n"
     ]
    }
   ],
   "source": [
    "embeddings_norm, vocab = get_embeddings_and_vocab('the_donald_1000_epochs')\n",
    "get_analogy('politics', 'trump', 'sports', embeddings_norm, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab16cc9d-4f62-4bb6-b439-69f6bae23a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2937524a-c759-43e2-b986-a7cfd8753584",
   "metadata": {},
   "source": [
    "#### CLustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1091400e-9230-45fe-ad68-009d3f859da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept1 = ['red', 'blue', 'green', 'black']\n",
    "concept2 = ['trump', 'biden', 'president', 'election']\n",
    "concept3 = ['sports', 'game', 'play', 'score']\n",
    "concept4 = ['good', 'bad', 'terrible', 'great']\n",
    "concept5 = ['car', 'bus', 'train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c92dd82-a675-4fdb-88bf-d59da5e94534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 10378\n"
     ]
    }
   ],
   "source": [
    "embeddings_norm, vocab = get_embeddings_and_vocab('the_donald_1000_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "46ed3d35-375f-4553-9f9a-8632bcc49356",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(embeddings_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "adc3b460-f4ca-47c8-a6a5-90c98890fe72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, ..., 1, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2341af19-157f-4d9f-849e-d9aaab8f5bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = embeddings_norm[vocab['red']].reshape(1, -1)\n",
    "kmeans.predict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b7088727-67c8-489c-8c82-f08db5f7d246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = embeddings_norm[vocab['blue']].reshape(1, -1)\n",
    "kmeans.predict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f1e26357-2d5a-4816-b73b-a369e6bf1683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = embeddings_norm[vocab['trump']].reshape(1, -1)\n",
    "kmeans.predict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b229b5a7-7f87-4ab1-9106-184dbe33971d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = embeddings_norm[vocab['biden']].reshape(1, -1)\n",
    "kmeans.predict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "69c234a9-4e29-40f3-a18e-53c0d3ee35af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3], dtype=int32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = embeddings_norm[vocab['bad']].reshape(1, -1)\n",
    "kmeans.predict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "92b23c73-bdc5-4e8e-8684-56bc7e6ab482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3], dtype=int32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = embeddings_norm[vocab['good']].reshape(1, -1)\n",
    "kmeans.predict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "799997f9-abd0-48b3-ac66-366367283d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = embeddings_norm[vocab['sports']].reshape(1, -1)\n",
    "kmeans.predict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "737ebd81-6174-4fa8-8370-cef2623950b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = embeddings_norm[vocab['car']].reshape(1, -1)\n",
    "kmeans.predict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1b479aee-1356-41d4-bcf0-da539ac31f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = embeddings_norm[vocab['bus']].reshape(1, -1)\n",
    "kmeans.predict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "baff856e-fa08-4d5c-a0aa-1de2f16a574d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = embeddings_norm[vocab['train']].reshape(1, -1)\n",
    "kmeans.predict(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee41fdd6-03a6-422b-9581-6b99898bfd12",
   "metadata": {},
   "source": [
    "#### Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cc198aba-581b-4e69-a19f-f07f5752f707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 10378\n"
     ]
    }
   ],
   "source": [
    "embeddings_norm, vocab = get_embeddings_and_vocab('the_donald_1000_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2091b2df-289d-4ea3-8963-9b9ecebe8b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bus 0.021671796683222055\n"
     ]
    }
   ],
   "source": [
    "get_outlier(['apple', 'orange', 'bus'], embeddings_norm, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "eb2e9c72-f891-469e-b820-3e00411bbe4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "president -0.03133737722722193\n"
     ]
    }
   ],
   "source": [
    "get_outlier(['sports', 'game', 'play', 'president'], embeddings_norm, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40efb3a4-06b1-477f-85e0-9cac4aa0f23a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
