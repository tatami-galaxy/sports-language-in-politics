{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a0872e-a17b-4ba2-bd6f-7fb73817f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# BeautifulSoup is used to remove html tags from the text\n",
    "from bs4 import BeautifulSoup \n",
    "import re # For regular expressions\n",
    "\n",
    "# Stopwords can be useful to undersand the semantics of the sentence.\n",
    "# Therefore stopwords are not removed while creating the word2vec model.\n",
    "# But they will be removed  while averaging feature vectors.\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from os.path import dirname, abspath\n",
    "from functools import partial\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8451d2e-1f67-4e16-b858-6165239bb15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIMENSION = 300\n",
    "EMBED_MAX_NORM = 1\n",
    "\n",
    "data_dir = \"/users/ujan/sports-language-in-politics/models/cbow/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11ac1d34-a946-445d-ac64-efd8b42d44e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available(): device = \"mps\"\n",
    "elif torch.cuda.is_available(): device = \"cuda\"\n",
    "else: device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe05003d-36b8-487a-8b32-cfd035233e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of CBOW model described in paper:\n",
    "    https://arxiv.org/abs/1301.3781\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super(CBOW_Model, self).__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=EMBED_DIMENSION,\n",
    "            max_norm=EMBED_MAX_NORM,\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=EMBED_DIMENSION,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.embeddings(inputs)\n",
    "        x = x.mean(axis=1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6c8f751-f5f1-4d0a-b829-910f33d9dd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   id\\tsentiment\\treview\n",
      "0      \"5814_8\"\\t1\\t\"With all this stuff going down a...\n",
      "1      \"2381_9\"\\t1\\t\"\\\"The Classic War of the Worlds\\...\n",
      "2      \"7759_3\"\\t0\\t\"The film starts with a manager (...\n",
      "3      \"3630_4\"\\t0\\t\"It must be assumed that those wh...\n",
      "4      \"9495_8\"\\t1\\t\"Superbly trashy and wondrously u...\n",
      "...                                                  ...\n",
      "24995  \"3453_3\"\\t0\\t\"It seems like more consideration...\n",
      "24996  \"5064_1\"\\t0\\t\"I don't believe they made this f...\n",
      "24997  \"10905_3\"\\t0\\t\"Guy is a loser. Can't get girls...\n",
      "24998  \"10194_3\"\\t0\\t\"This 30 minute documentary Bu√±u...\n",
      "24999  \"8478_8\"\\t1\\t\"I saw this movie as a child and ...\n",
      "\n",
      "[25000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/users/ujan/Downloads/Sentiment_Analysis_Dataset.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7563944d-8d1f-4bf4-bdb2-16fb663edf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "sents = []\n",
    "revs = []\n",
    "for item in df['id\\tsentiment\\treview']:\n",
    "    id, sent, rev = item.split('\\t')\n",
    "    sent = int(sent)\n",
    "    ids.append(id)\n",
    "    sents.append(sent)\n",
    "    revs.append(rev)\n",
    "\n",
    "df = pd.DataFrame.from_dict({'id':ids, 'sentiment':sents, 'reviews':revs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6acededd-6c46-4ca2-bef7-1e750334c727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                            reviews\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f029d7a-53a2-4b51-86a4-504016310885",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews=df.iloc[:,2].values\n",
    "labels=df.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d09b2f9b-705c-4032-b433-d691469ae978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/by/rktr_w596p97pmt8_cbknvs80000gn/T/ipykernel_16098/3324077737.py:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def parseHtml(html):\n",
    "  soup = BeautifulSoup(html, 'html.parser')\n",
    "  return soup.get_text()\n",
    "\n",
    "def removeDigits(string):\n",
    "  for i in range(10):\n",
    "    string=string.replace(str(i),' ')\n",
    "  return string\n",
    "\n",
    "#removing html\n",
    "reviews=list(map(parseHtml, reviews))\n",
    "\n",
    "#removing digits\n",
    "reviews=list(map(removeDigits, reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a210cd1-303e-4020-9da2-96d0b1d6c719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /users/ujan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#tokenizing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "tokenizedText=[nltk.word_tokenize(item) for item in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "452c366a-6c32-4f06-92fa-621c9ec93ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\,'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\,'\n",
      "/var/folders/by/rktr_w596p97pmt8_cbknvs80000gn/T/ipykernel_16098/959386291.py:2: SyntaxWarning: invalid escape sequence '\\,'\n",
      "  punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n"
     ]
    }
   ],
   "source": [
    "#removing punctuation\n",
    "punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n",
    "tokenizedText = [[word for word in review if word not in punc] for review in tokenizedText]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bb0daa2-5e53-4dc6-82cf-ac15cd4f4516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#splitting the Dataset into train and test set\n",
    "totalRows=len(tokenizedText)\n",
    "\n",
    "splitRatio=0.75\n",
    "splitPoint=int(splitRatio*totalRows)\n",
    "\n",
    "trainReviews=tokenizedText[:splitPoint]\n",
    "trainLabels=labels[:splitPoint]\n",
    "testReviews=tokenizedText[splitPoint:]\n",
    "testLabels=labels[splitPoint:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6425259-0623-4d2f-bc3f-a6fa7f5be6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_and_vocab(sub_name):\n",
    "    \n",
    "    vocab = torch.load(data_dir+sub_name+\"_vocab.pt\")\n",
    "    print('vocab size: {}'.format(len(vocab.get_itos())))\n",
    "    model = CBOW_Model(vocab_size=len(vocab.get_itos()))\n",
    "    model = torch.load(data_dir+sub_name+\"_model.pt\", map_location=device)\n",
    "\n",
    "    def normalization(embeddings):\n",
    "        norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\n",
    "        norms = np.reshape(norms, (len(norms), 1))\n",
    "        embeddings_norm = embeddings / norms\n",
    "        return embeddings_norm\n",
    "    \n",
    "    # embedding from first model layer\n",
    "    embeddings = list(model.parameters())[0].cpu().detach().numpy()\n",
    "    \n",
    "    # normalization\n",
    "    embeddings_norm = normalization(embeddings)\n",
    "\n",
    "    return embeddings_norm, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c1bb865-833b-435d-9083-6678dd27def0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 9269\n"
     ]
    }
   ],
   "source": [
    "embeddings_norm, vocab = get_embeddings_and_vocab('politics_2015_9_12_100k_samples') \n",
    "#embeddings_norm, vocab = get_embeddings_and_vocab('politics_2015_10_35k_samples') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51236403-a037-477f-ad7d-9668bfe520b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectors(dataset, embeddings_norm, vocab):\n",
    "    vocab_set = set(vocab.get_itos())\n",
    "    singleDataItemEmbedding=np.zeros(EMBED_DIMENSION)\n",
    "    vectors=[]\n",
    "    for dataItem in dataset:\n",
    "        wordCount=0\n",
    "        for word in dataItem:\n",
    "            if word in vocab_set:\n",
    "                singleDataItemEmbedding = singleDataItemEmbedding + embeddings_norm[vocab[word]]\n",
    "                wordCount=wordCount+1\n",
    "\n",
    "        if wordCount > 0:\n",
    "            singleDataItemEmbedding=singleDataItemEmbedding/wordCount  \n",
    "        vectors.append(singleDataItemEmbedding)\n",
    "    return vectors\n",
    "\n",
    "trainReviewVectors=getVectors(trainReviews, embeddings_norm, vocab)\n",
    "testReviewVectors=getVectors(testReviews, embeddings_norm, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ffe8e761-7a1e-45ad-883b-afc435a617df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import sys\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def printResults(y_true, y_predicted):\n",
    "  print(\"Accuracy= \", accuracy_score(y_true, y_predicted))\n",
    "\n",
    "  columns=['false', 'true']\n",
    "  #plot_confusion_matrix_from_data(y_true, y_predicted, columns)\n",
    "\n",
    "  precision, recall, fscore, support = score(y_true, y_predicted)\n",
    "\n",
    "  print('###########################################')\n",
    "  print('precision: {}'.format(precision))  \n",
    "  print('recall: {}'.format(recall))\n",
    "  print('fscore: {}'.format(fscore))\n",
    "  print('support: {}'.format(support))\n",
    "  print('###########################################3')\n",
    "\n",
    "  print('Macro F1 ',f1_score(y_true, y_predicted, average='macro'))\n",
    "\n",
    "  print('Micro F1 ', f1_score(y_true, y_predicted, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d186b727-3448-4057-93dc-739b077ec176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################RESULTS OF NAIVE BAYES CLASSIFIER##################\n",
      "Accuracy=  0.56128\n",
      "###########################################\n",
      "precision: [0.1369201  0.97997457]\n",
      "recall: [0.87090164 0.53505727]\n",
      "fscore: [0.23663697 0.6921868 ]\n",
      "support: [ 488 5762]\n",
      "###########################################3\n",
      "Macro F1  0.46441188471512307\n",
      "Micro F1  0.56128\n"
     ]
    }
   ],
   "source": [
    "#naive bayes\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clfNB = MultinomialNB()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaledTrainX= scaler.fit_transform(trainReviewVectors)\n",
    "scaledTestX = scaler.fit_transform(testReviewVectors)\n",
    "clfNB.fit(scaledTrainX, trainLabels)\n",
    "\n",
    "#test naive bayes accuracy\n",
    "testLabelsPredicted=list(clfNB.predict(scaledTestX))\n",
    "\n",
    "#print results\n",
    "print(\"####################RESULTS OF NAIVE BAYES CLASSIFIER##################\")\n",
    "printResults(testLabelsPredicted, testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7015491-548a-43ee-9436-94927850a7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################RESULTS OF NEURAL NETWORK CLASSIFIER##################\n",
      "Accuracy=  0.796\n",
      "###########################################\n",
      "precision: [0.80670103 0.78544183]\n",
      "recall: [0.78766908 0.8046239 ]\n",
      "fscore: [0.79707146 0.79491716]\n",
      "support: [3179 3071]\n",
      "###########################################3\n",
      "Macro F1  0.7959943126478457\n",
      "Micro F1  0.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ujan/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#neural network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clfMLP = MLPClassifier(hidden_layer_sizes=(10, 10, 10))\n",
    "clfMLP.fit(trainReviewVectors, trainLabels)\n",
    "  \n",
    "testLabelsPredicted=list(clfMLP.predict(testReviewVectors))\n",
    "\n",
    "#print results\n",
    "print(\"####################RESULTS OF NEURAL NETWORK CLASSIFIER##################\")\n",
    "printResults(testLabelsPredicted, testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e18981-aaee-4009-a457-9caa632caac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e5bcd70-23a7-4384-a6f8-3e22d95a0558",
   "metadata": {},
   "outputs": [],
   "source": [
    "## shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "314087d5-dbfd-47c7-94c8-e86f9131704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(trainReviewVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3df975df-967e-4acf-9e88-445bdf8d3221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################RESULTS OF NEURAL NETWORK CLASSIFIER##################\n",
      "Accuracy=  0.5136\n",
      "###########################################\n",
      "precision: [0.59117268 0.43706294]\n",
      "recall: [0.5088741  0.52004539]\n",
      "fscore: [0.54694486 0.47495682]\n",
      "support: [3606 2644]\n",
      "###########################################3\n",
      "Macro F1  0.5109508402636748\n",
      "Micro F1  0.5136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ujan/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#neural network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clfMLP = MLPClassifier(hidden_layer_sizes=(10, 10, 10))\n",
    "clfMLP.fit(trainReviewVectors, trainLabels)\n",
    "  \n",
    "testLabelsPredicted=list(clfMLP.predict(testReviewVectors))\n",
    "\n",
    "#print results\n",
    "print(\"####################RESULTS OF NEURAL NETWORK CLASSIFIER##################\")\n",
    "printResults(testLabelsPredicted, testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e546f2-efa7-41b0-a05f-7c70e5df4629",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
